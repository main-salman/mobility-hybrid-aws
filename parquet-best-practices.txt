Data Lake Fundamentals, Apache Iceberg and Parquet in 60 minutes on DataExpert.io - YouTube
https://www.youtube.com/watch?v=hFFP2OYFlTA

Transcript:
(0:00) data lakes and file formats today so let's let's dig into it so the main things we're going to be talking about today are one what is a data Lake two what is compression compression is like this magical thing where it's like wow the data is now smaller but like what causes the data to actually be smaller that's the thing that's like some people might not understand what actually happens underneath the hood there we're going to go into some rather intricate detail about uh compression so you can understand more around what is actually
(0:31) happening underneath the hood when uh compression is happening and then we're going to be talking about how parket file format is amazing uh I use Park in particularly at Netflix and Airbnb Park was what we love to use a lot and uh the last thing we're going to be talking about is why should we partition our data sets and partitioning data sets especially on date is a very very powerful thing that allows your data and the data in your Lake to be a lot more manageable so yeah let's dig into it so what is a data Lake uh simply put
(1:10) a data lake is a set of files on someone else's computer whether that be Jeff bezos's computer or SAA nadella's computer or what Sergey brin's computer like most of the time y one y'all is the data lake is usually a bunch of files in the cloud uh and usually it's going to either be on AWS azure or gcp it's going to be some sort of cloud compute way of doing things uh and those are going to be kind of like the data Lake way of doing things obviously there was like in the past they had Hadoop and you could do your old kind of
(1:43) like setting it up your yourself and having like the replication factors and all that stuff but nowadays you can think of it as a data lake is set files in the cloud and it's really that straightforward but then it's like one of the things that uh with this definition that trips people up a lot is there's no constraints it's just a bunch of files in the cloud so it's like not like postgress in that way and it's not like a data warehouse in that way that like you can literally put whatever the hell you want and that's why like
(2:17) with the data Lake like uh new paradigm and the new data Lake Paradigm what we want to be doing differently is having some sort of rapper on top so the three big competitors in that space right now is you have Apache Iceberg Delta Lake and you have huie those are going to be the three kind of competitors in uh the abstraction on top of the files in cloud so that it's not just like a data swamp because I think that that's a very common uh thing that happens is because there no constraints on the files that you put in
(2:52) the cloud people just put whatever the hell they want in the cloud and put whatever the hell they want in lake and then the quality goes down and then you have all sorts of other problems that can happen when you aren't managing your data link correctly but remember there's nothing magical about a data Lake it literally is everyone's renting Jeff bezos's computer and we just put our data on Jeff Bas with his computer that's literally all it is let's go to the next slide so what is compression so compression is a way of
(3:25) reducing the size of data and generally speaking there's two types of compression you have lossy and lossless compression so uh in the data engineering space uh lossy compression is just not the way to go because what you get in what you uh compression gains you lose in quality because now what if the data is just wrong when you decompress it because the data is a little bit different uh lossy compression is a lot more common when you're like streaming analog data or like you know in video I'm sure all of you have tried to send a text message
(4:02) with an image on it and then that went across uh the SMS and then it was just absolutely trash because uh SMS when you send an image across SMS it's a lossy compression algorithm so a lot of the details of your image gets lost and like it you can kind of still make out but it's bad and so the lossy compression algorithms are all over on the internet especially when you're talking about like rich media video image audio processing in those areas where like you have a lot of dead space in the video that can be
(4:36) compressed down but a lot of times those are like that's just how it works but like in when you're working with data like that and it's a video lot of the time like you don't need that to be the highest quality because you're just watching the video you're not really making a ton of decisions off it right but then there's also another type of compression called lossless now lossless compression is really really awesome so lossless compression is a way that you can reduce your data size without losing any anything you there's no drawback to
(5:11) lossless compression and that's what we're going to be mostly focusing on today in the lab and also this presentation lossy compression is not as important for data engineering so um remember you should only be using lossless uh don't use lossy compression it's not worth it but with lossless compression you can Google it if you Google lossless compression techniques there's like 12 or 13 of them there's really only one that matters at all in data engineering and that's going to be run length en coding coding is super powerful and it's one of
(5:47) those things that is uh supported by Park file format and a couple of the other ones I don't know if you heard of orc like optimized row column there format uh both of those support run l L and coding and run length is in my opinion the most important and Powerful aspect of Park file format uh we're going to be talking more about some of the massive wins that I got at Airbnb leveraging this compression technique so yeah let's dig a little bit deeper so how does run length en coding work so run like coding works by
(6:26) compressing data that is repeated so imagine if you have a bunch of repeated data then what will happen is it just it will just not repeat and will just nullify the values that are repeated and then it shrinks the data set quite dramatically so imagine we had this table it's called likes by country and gender and we have a date country a gender and uh the number of likes that were in that country gender by on that date so imagine we had this data set and let's go over to another slide here so uh data in this
(7:02) data set might look something like this where you have 2024 January 1 US mail number of likes and uh one the key things I want you to think about here is you see on the left side of that data set you see how like we have four 20241 01s repeated and then we also in after that we have us repeated twice and India repeated twice so this is a very common that happens in data sets when you have repeated data especially when you're building like olap cubes that have like where you cut by a couple Dimensions uh at the same time because
(7:40) then you have all the combinations of those dimensions and then the com like the combinations will uh compress differently so what does run length encoding do to this data set compress it so what it does essentially is uh you'll see everything that is has a strike through what it does is PAR file format and run length en coding will null those out and remove that data then at the top of each one so you see this 2024 101 it will say four so that means that we have four in a row of one and then the rest of these uh dates
(8:18) are nullified and that data is removed from the data set so it dramatically reduces the data set volume and you'll see with like the us so we have a run of two here we have us and then we say two because uh again can save some space there even when the the streak is only too long obviously like if you have a streak of one run length en coding doesn't work it do anything so one of the most important things that you got to remember when want to leverage run lengthen coding compress your data sets is how data sorted right because sorting is what's
(8:57) going to matter the most because if we sort Ed differently if we sorted this data set differently we might get different compression and uh the streaks would look differently right because imagine if instead we sorted on gender gender might actually be better here because then all the males will be together and all the females there's like there's only like a couple values that are even possible for gender so you think about like when a dimension has a low cardinality that is going to be where you want to sort on that column
(9:26) first and then go down the line that's how you get the best compression and we're going to be over that deeply in the lab today so make sure you get a data expert. account so that we can get ready uh to do all this stuff so one of the things I key thing here that I want you to see is the more repeated values in the data set more savings that you get so how much run length en coding actually compresses your data set really does depend on the data set where some data sets might see like like a 10 or 20 30% compression
(10:02) gain others are going to see a dramatically higher compression gain it really just depends on how many dup like repeated values and how you sort your data so yeah let's go to the next slide so remember uh when you're sorting your data you want to uh your needs to be sorted by lowest cardinality to highest cardinality dimensions that's going to be how you can uh really get um the best compression so I want to talk about an example here that I did at Airbnb so one of my data sets that I had at Airbnb was for availability and what it was
(10:40) had a listing and then it had 400 rows for the next 400 Days of availability so it's like you have the listing ID date and then there's 400 rows for every listing ID so you could imagine if we made sure that all the listings were grouped together all of those listing IDs would be compressed down they all be compressed down into like just one ID right because you would have 400 repeated values and so all those would roll up into one value and so of the things I noticed was that like just by adding literally one line of
(11:16) code and it was just a sort all was a little sort line of code uh to the availability data set it went from being 10 gigabytes to being3 gigabytes and so that was a massive massive win for Airbnb in terms of space because it's 10 gigabytes a day and they needed to hold on the history over uh many years like over a decade of time they needed to hold on to those 10 gigabytes a day but now it's like more than 10x savings right like it was a 95% reduction in size even more than that and so one of the things that this allowed um airb
(11:55) Airbnb to do was all the pipelines that depended on this data set they started running a lot faster they started um having a lot fewer errors because the data set is just a lot more compressed and uh because of how runlength en coding works so keeping in mind that runlength en coding does not apply to all file formats that's something important to remember so like for example if you have your data Lake and you're just throwing CSV files into your data Lake guess what you there's nothing you can do like the 10 gigabytes
(12:32) like uh the Sorting there's no runlength en coding in CSV so you can't use CSV is kind of a dumb data type I don't really like it as much because it doesn't leverage any of these compression uh things at all so we're going to go a little bit deeper here talk more about Park specifically because that's a very important part of your data Lake formatting so let's go to the next slide so um why should we use Park park really um has a couple benefits uh the biggest one that I just hammered into y'all's skull is uh run length en
(13:10) coding so run length en coding is going to be the biggest benefit that you get from uh using par another one that's also very good is it's colner instead of row oriented whereas CSV files are row oriented and I just hate row oriented data storage row is don't like it especially for data Lake it's a row oriented is little bit better in like the transactional postgress like uh relational database World there it's like a little bit better but like in this world like in the data Lake you better be using columnar because
(13:48) colner is going to be way better and here's why colner is better so imagine you are looking at this data set and this data set has 10 columns but you only need two of The Columns so if it's column there you can just select those two columns and then the other eight will just be ignored and you don't have to read in the data they're just left the data is just left and you don't have to it's not read in you just ignore those columns and you just select that the data that you need as opposed to all of the data and so when you write your
(14:19) data in par format it allows the consumer of your data to just select the data that they need as opposed to selecting all of the data and so this can make the analysis dramatically quicker like dramatically quicker depending on like how many columns they're selecting and what they're counting by and aggregating because then when you're selecting the right columns bada being boom you're good to go and you're you just it minimizes the amount of data that you actually read Because keeping in mind keeping in that when you
(14:57) when you think about Cloud cost right there's a couple different ways to think about Cloud costs when you're working in the data Lake world uh one is the cost to hold onto data on Jeff beas's computer so like S3 storage cost that's one S3 storage cost is actually pretty damn cheap even a paby paby whatever like even that's pretty reasonably priced so storage is actually pretty cheap um ec2 like which is going to be your uh compute when you're actually running your compute uh instances that is a little bit more
(15:30) expensive but there's one area in Cloud cost that is the most expensive and that's going to be Ingress and egress costs of reading data out S3 and writing data to S3 that is going be where you hit that get the biggest freaking like penalty in terms of cost like so you know in the example that I gave where I got that massive savings for uh the availability data set at Airbnb most of that savings was not in storage because that's just like one data set because where we got the big savings is that there were hundreds of
(16:07) pipelines that read in data set so instead of reading in 10 gigabytes a day now it's reading in3 gigabytes a day and and then that's times a 100 or 200 what however many Downstream pipelines there are and then that is you got that's the that's the thing you got to remember as a data engineer is a lot of the times when you're creating data sets like it's not about the storage even about the compute to generate data set it's about how is this data read how is this data set consumed it consumed by other pipelines is it
(16:40) consumed by analysts all of those things and it's like if you have a data set that's consumed by analysts it better be colner like have a colner data set that your analyst is uh querying because it's like then they get to decide how much data they consume and then you minimize the Ingress and egress cost of reading and writing data from S3 so that's why columnar data is just the way to go it's amazing and obviously run length en coding really good uh the last thing I would want to say is it's everywhere par is what I used
(17:15) exclusively at Netflix and Airbnb um Facebook was interesting at Facebook they there was like the big debate about whether or not to use orc park there was like a big battle uh and orc I used orc more than par at those Facebook but that was also a really long time ago that was like eight years so par has gotten a lot more adoption and I bet it has more adoption at Facebook now as well so anyways that is gonna be how par is just so good okay let's talk a little bit about partitioning partitioning is another very important concept uh in the data
(17:56) Lake world so remember we said that uh a data lake is just files on somebody else's computer that's all it is is so partitioning is putting folders for those files it's giving some structure so it's not just like you have this one big beefy file that has all of the data and you just have to always play with this gigantic file every time you can put that file into different folders and then that allows you to select just the data that you need usually this is based on time so we're going to go over a really solid
(18:35) example in the lab today over like how partitioning is really powerful on like what it will do to uh minimize the data that you consume so keeping in mind all of this stuff together is going to be what contributes to data minimization right so colum you minimize the columns run length encoding you minimize the amount of data that's being being read in and partitioning is you minimize the number of rows that is being read in and so that's what partitioning is going to do so you use all if you use all three of these
(19:10) Concepts together that's when your analysts are going to be very happy with your data Lake performance and your Cloud costs all of that stuff so a key thing to remember here about partitioning is It's usually the case that uh your data is going to be partitioned on Dat like some sort of time Dimension is going to be the most common way that your data is partitioned but not always that's not always the only partitioning uh another uh great example is if you have another dimension that's kind of low cardinality you can put that
(19:46) one in there as well like for example when I worked at Facebook we would partition uh all the notification data would be partitioned on date but it would also be partitioned on channel so then like if you want wanted to read the SMS notifications without having to read all the push notifications you could look at just SMS and it would be fast or email notifications and not push or just push right you can just pick and choose if there is another kind of obvious logical grouping that you want to use for partitioning that's another
(20:21) very powerful way to go so uh some things I want to caution y'all about with partitioning is you don't want to partition on um a column that is high cardinality like if your column is an ID like a user or uh listing ID anything that has like millions or tens of thousands cardinality you don't want to partition on that that's just gonna that's going to create way too many folders and that's going to cause S3 to have a different problem so what happens in that case when you have too many files and too um folders
(20:58) is S3 uh there's a very expensive operation in S3 called file list and so what file list does is it asks S3 give me all of the files that are part this right give me all the files that are part of Jeff bezos's computer and um if you have all these folders then that can take a long time and that then that can be that file list operation then be the bottleneck for your queries if you partition too much so that's the thing about partitioning there is like not partitioning is bad because then you have like one gigantic file that is kind
(21:37) of hard to work with partitioning too much has a different problem because then you have too many small files that you're working with that has causes a different set of problems so you want to be kind of in the middle ground there where it's like uh maybe like one folder per day and then maybe adding one other uh thing especially if you have a large amount of data then adding another partition there keeping in mind when you partition on more than one column that the the second column was called a sub partition just to kind of give you the
(22:11) lingo of like how things are actually said and how things are working so that is essentially uh what I wanted to cover in today's presentation and so we are going to be transitioning over the lab here in just a second so uh let's go to the last slide of this so what we're going to do in the lab today is we're going to be covering uh run length encoding data late compression with trino and make sure you have a data expert.
(22:39) i account and go to then log in you gotta be logged in and then go to data expert.i classroom back Zack Wilson and that's where you can follow along live and you can get notifications of all the queries that I run so you can kind of catch up and you can stay up with me so here I'm gonna we're g to switch over here to that um window now let's go with uh yeah there we I think one second I just want to get this um to focus can we get that okay there we go that's focused so now what we have here is we're going to be working with uh this classroom just want
(23:19) to go over the classroom um and how this works because we will be creating data sets today and if you uh don't follow the instructions the right way here you will not be able to create data sets so when you create data sets in expert.i you have to create them inside of your schema and is based on the username that you picked when made your account and so for me I have Zach Wilson as my username and so what we're going to do is we're mostly to be working with um MBA game details and M and MBA games those are going to
(23:53) be the two big tables that we're going to be working with today so one of the things I want to show y'all is what happens when we run this query so I'm going to just run see like what does NBA game details look like so this is the create table statement so you'll see here here it is and keeping in mind this is a data lake that is uh managed by Apache Iceberg so iceberg is the metadata metadata layer here that manages all of this data and uh you'll see here is our table it has a bunch of columns big in varar all that
(24:29) kind of stuff and you'll see it is a parket format one of the things you might notice here is this table not partitioned it's not partitioned at all so what we want to do here is let's go ahead and create another table here that is partitioned and we're going to partition it on season because I think season is a good column to partition this on but one of the things you might notice here is season actually not a column in this table uh you'll see there's another table here called MBA games and that's where we're going to
(25:03) have that so the idea here is we're going to try out a couple different uh compression techniques and we're going to be creating a new uh schema here so we're GNA say create table and I'm call this uh Zach Wilson I'm gonna call this NBA game details I'm gonna call this NBA game details sorted so for your table make sure that uh this is your username so that you can actually uh play along here so then we can say player name uh is going to be a varar we're not going to like do all of these columns because that's like a pain but
(25:36) what we can do here is have player name and then I think another column here that would be useful is uh Team abbreviation I like team abbreviation that could be a varar and then uh let's go ahead and get the uh kind of points and the metric columns so we're going to say points as a double we're going to say rebounds as a double we're going to say steals as a double we're going to say assists um as a double right because uh like I know these are actually integers but like they're stored as doubles in here so we're going to keep them as doubles and
(26:13) so now what we have is uh we're getting there but we need a couple other things in here we're going to need um game ID and game in this case is a big int and then we're going to need season which is an integer and then we also have game date which is a and then so this is going to be our starting kind of uh table that we're going to be working with but now we need to specify that as Park so if you use wi here that's how you can specify um kind of the table formatting so in this case we say format equals can say park and then we want to say
(26:48) partitioning equals and then how partitioning works is it's an array uh we we specify as an array of values and then inside of this array we put the values that we care about so in this case we're just going to partition on season and then the last one we want to do here is we're going to sort so we have to specify how we're sorting this table I personally like sorting think the the right way to do this is we should sort by player name and game date I think that's probably going to be a very good way to sort this data but we
(27:22) are going to compare we're compare like uh sorted versus unsorted all that kind of stuff so now this is a pretty solid um pretty query I think that we can uh run run with this so you'll see get a notification here that of the what I actually ran and so now we have access to this table right just got created so now what we want to do is we need to get all of these columns right so if we say like select star from boot camp.
(27:56)  mbaam details and we're going to call this GD and then we need to get that season and game date columns and those are in boot camp MBA games so let's go ahead and uh join here we're gonna say boot camp. NBA gamg and this is gonna be on g. game ID equals g. game ID so now the goal here is we're just trying to get all of these columns right so first we got to do GD dopler name g.
(28:26)  team abbreviation then we have uh points so in this case it's GD points is just uh pts and we're going to call this as points and we have GD dot what's Reb is it just okay it's Reb as rebounds and then we have steals GD do steel as Steals and then um assists GD do assist as assists then what else we got game ID so this is going to be GD do game ID then we have um season so season's actually coming from the other table we have that in the games table and then one of the things that you'll see is in this boot camp NBA games one of the
(29:11) things I want to show y'all real quick is how this editor kind of works so if I say like select star from NBA games this I can actually run the selection so we can actually see what the hell is in that uh table so we don't have to like make assumptions and you'll see here we have game date ES T so that's what we're going to want do but we're going call we're going to wrap that so going to say date and we're going g.
(29:35)  game dat as game date because I don't think think EST is a stupid um thing to put in there so now this is going to be uh a pretty good uh representation of what we're trying to do here so if we run this you'll see uh like this should give us essentially the table format that we're looking for you'll see it's not really sorted but that's fine because the Sorting will happen on right so now what we want to do is we're going to take MBA game details and we're going to say insert into Zach Wilson NBA game details sorted and then we're gonna
(30:14) we're just gonna run this guy so now this is gonna uh run and we we'll be able to get our sorted data okay so that just created 669 th000 rows of data so this is obviously not the smallest data set in the world right it's actually uh we're getting pretty big here so now what I want to show y'all is if we say select star from uh we're just gonna gonna select star from this guy real quick and we're gonna read in this uh this this table here okay so you see now how it's like Aaron Brooks Aaron Aaron Brooks and it's like
(30:58) sorted right by all of his games in a row and then Aaron gray gray right so you have all of them sorted in a row but you'll see it's actually only like 2007 because remember we're partitioning on season so it's not like all of their games in a row it's just all of their games in that one season in a row so one of the things I want to show you is we made a very big difference compared to this old boot camp NBA games details table and this current one so I want to show you how do this in iceberg so what you can do is you there's a special
(31:38) thing here where you can actually query the files that were actually created where what you do is wrap it in quotes and then you put dollar files at the end of the table name and then that allows you to actually see the data that was created here so you'll see are all of the parquet files see here's 2022 season 2021 or 2012 and the paret file where it is sitting so this is remember this is just Jeff bezos's computer right and then we have we're sitting in this folder and then all of these additional folders right and
(32:10) you'll see that partitioning what it does is it creates another layer of a folder here that's what it does so I want to compare that and let y'all see so if we say select star from and boot camp. NBA game details but then we put this in quotes and then we say doll five this is going to be a significantly fewer number of files because we don't have that partitioning so you'll see it's literally one file just big ass file at this location you'll see in here you see how there's no folder folder in here no additional like C's
(32:45) and equals folder it's just like here is your gigantic 600,000 row parquet file have fun bro right and you'll see uh you with with um Iceberg and all this stuff you get a lot of very interesting data right so you see for example here we have record count and then the file size in bytes so one of the things that is going to be happening here is now that our data is sorted this file size in bytes is going to be a lot smaller overall so you'll see here what we got um what we're going to do is to call this sum file size invites right
(33:23) and then we're going to call this a new and then here we're going to say old uh some file size in byes and then we're going to call this as byes is byes we want to say Union all here obviously we didn't keep all of the columns but you'll see here uh we can look and we can actually compare contrast all of them together right so you'll see in this case like we are dramatically lower here obviously we didn't uh um select all the columns so we did also lose some data there but that's what I want to illustrate y'all next is we want to I show
(33:59) you how sorting actually does make a difference so what we're going to do now is we're going to go up this NBA game details sorted and we're going to make a new table here called NBA game details unsorted and the only thing we're going to do is we're going to remove this sorted by attribute and the rest of this is going to be exactly the same so then what we're going to do is create this table that is does not have the nice sort value right so now what we can do is we can very easily copy that data and we can do this insert query again very
(34:36) quickly right so now this is going to run and we're going to write a billion trillion rows into the data Lake and then you'll see how that like Works in terms of building out a lot stuff so you see here now we have wrote all the data here so now in this case I want to query all of the files but I'm going to do this as unsorted so is unsorted and this is sorted so going to show you how much we save remember like uh what I was talking about with run length and coding length and coding sorting is very very connected so let's see how much we
(35:11) save when we sort our data set um oh this is Zach Wilson that's why Zach Wilson not boot camp so now this is going to show us um so you see we got a little tiny bit of savings here and so that might be an indicator to us that there might be a better way to sort the data than what we currently are doing that is one po potential possibility but you see that in this case the unsorted data is actually it's saying is actually smaller and so one of the reasons for why that could be the case is if we sort by player name we might be uh exploding
(35:57) another column here and that's actually what's going on so let's go ahead and uh query that data set real quick so if we say select star from let's see what the this actually looks like what is what is the unsorted data look like here so interesting generally speaking this uh okay so you'll see here I think what happened is in this case we have uh the this player okay then we have game ID and season those actually should be like that's very very interesting that actually didn't uh didn't do what I wanted let's
(36:39) um okay so what I think happened here in this case is the Sorting uh it's actually the compression here is happening because of the fact that uh the the natural order of data is actually better at compressing it than sorting it which the only way that would happen is if there some sort of column in here where there's like a lot of people who have a lot of zeros right you see all these people who have a lot of zeros and they're like clustered together all the people with zeros are clustered together and that would mean
(37:13) that these columns Are all uh not these double columns are uh CL all these zeros are clustered together so that's one way to that you can see um sometimes uh when you sort things it can actually make the data bigger because uh because these zeros are all like clust they're more clustered together in this case than they are with uh when you sort by player name so if we go into like how that works right let's go ahead and try one more example here where let's do um call this sorted and then we're GNA say um going to
(37:52) call this sorted points so in case all we're going to do is say is say sorted by equals array and we're just going to sort on points here to see what that does right because one of the whole things here that I'm trying to illustrate to y'all as data Engineers is it's very common that when you are working with things and you're especially when you're working with these optimization problems you can get unexpected values here so an unexpected results so if we go ahead and run this sorted points this is going to give us
(38:27) um a new kind of thing where I think what this will do is going to give us what we would expect hopefully uh and that will kind of like because run length en coding is what this all about right okay there we go so now let's look at uh sorted points so if we say we're going to add sorted points the files here we're going to compare the two file sizes in bytes okay so you see there we go the the natural order of the data there was really strange so this is illustrating it better so that you can kind of see when uh that was like not
(39:07) sorting it at all that was like a fluke but you see here like if depending on how you sort the data get a very big difference in the file size so um and that was like the natural order of this data is very strange and I'm I want to learn more about like what's going on there but like you'll see here this is going to be we're call this we go this is sorted by points and this is um sorted by player name right let's uh let's rerun this query so that that's like more obvious because it's not actually unsorted that's not what I ran W let uh
(39:41) make that go away then uh so if we run this selection here there we go now this will be sorted by player name and uh sorted by points so you'll see now we can see okay we have sorted by player name sorted by points and you see when we sort by player name we get dramatically different uh size in the data and this comes back to the Run length encoding that is inherent with a park file format so that's essentially what we're trying to do here is trying to find the optimum sorting for the data set and this is so that
(40:16) we can compress it down and make as small as possible so that is going to be the main way that you can get some very big wins uh when you're working with Park file format so the other thing I wanted to show y'all is like so now we have this NBA game details sorted let's go ahead and um I want to show you another really cool thing you can do with trino so you can say show stats for and then what if we say we're gonna select star and we're gonna say from Zack wilson.
(40:51)  MBA game details sorted where season equals 2022 so if we run this query this is a great way to see how much data is scanned so uh you'll see here uh this is saying we are this is how much uh of each column that we're going to be scanning is right and the distinct values that's interesting that these are all null some of this is null because of the fact that uh it's um it it didn't like register all of the data sizes because these are all like integers and stuff whereas this is a string these are strings so one of the things you can see
(41:31) is we have a partition here right but then if we say boot camp NBA game details oh this doesn't have season though but like if we run this you'll see uh this one is going to have significantly more right you see this the the data volumes and sizes can be very much larger right and that's a very common thing that can happen when you uh are working with different data sets when you're wanting to like understand how much IO is happening for your data this is a great way to do it is like with show stats for because this
(42:06) is going to show you exactly how many bytes is like actually coming through when you're reading this data set so um that's going to be another great example of where you can show how partitioning is going to help a lot like so I wanted to go over one more example here because I think that there is another example here that is probably better will illustrate this again because I don't want to confuse y'all in terms of like what happened there with like some of the Sorting but we're gonna actually look at this other table here called
(42:36) actor films so if we look at um boot camp actor films this table is uh pretty cool so in here you have like an actor you have a film year so this table is partitioned on year so one of the things I wanted to do though is wanted to compare two different ways that this could work right because you could see how you can sort on film or you can sort on actor right you could which one which comes first so let's go ahead and create another table here we're going to call this Zach Wilson we call this actor films sorted and then this is going to be uh
(43:17) gonna have an actor varar we a film varar and we have a year which is an integer and then um really I don't want the rest of those columns I think that that's all we really want and then in this case we're going to say wi and we're going to say format equals parquet and then we have partitioning is still uh the same right where we have a year as the partitioning and then in this case we're going to say sorted by and then the array here we want to try actor and then film but we also want to so I'm going say this is
(43:50) going sorted by actor we're going to create two tables here real quick we're going to create this table and then uh this one's very easy to like insert into right so okay we did that one so say insert into uh this table then we're going to say select actor film Year from boot camp. actor films so this is just going to show you like okay is there more a great way to think about this is like to sort by actor or film first is like are there more are there more actors per film or are there more films per actor that's
(44:30) the question that you want to be thinking about here my guess is that there's more actors per film but like I think that that's it really depends I think that there's definitely going to be some that are G to be the other way around but like so we're going to do sorted by actor and then we're g to sort by film as well so we're gonna say sorted by film where in that case what we're going to do is we're GNA sort by film and then actor we we'll we'll see which one uh actually is the better way to go because I think it this really illustrates what
(45:02) the cardinality looks like between the two right so there we go and then just want to do sorted by film we're going to insert that into the data set here oh did I not forgot to actually run the selection so there we go are running that selection now we will get our kind of data sets and we can see so now if we uh let's just look at select star from this guy but we're gonna be going back into the the files right just so we can kind of understand what is going on here and then uh let's just do the whole thing we
(45:38) can say file size in bites right and then we want to uh do another one here and this is sorted by actor we say Union all then this is uh Say by film and then this one here is by actor and then we can then run this uh to see what is the better way to sort right because one of these is probably the better way to sort so you'll see in this case it doesn't matter that much but because if you think about it like probably makes sense for like uh the average actor probably has 20 or 30 um films throughout their career whereas the
(46:25) average film probably has 20 to 30 actors that's why this is very close right these are actually like the compression gains that you get whether you sort by film or whether you actor it doesn't matter that much so that's a good sign that uh like but you but you see that like my initial hunch there of like sorting by actor uh like that like if you sort of buy film it would you get a little bit more compression so that was uh correct right but like this is the idea behind like how uh if you want to manage your data and you want to do things the
(47:06) right way like you need to be thinking about this stuff like the number of data Engineers who think about this is very very very low most people think they just write their damn squl and they just are like okay I wrote my insert into wrote my create table and that's it now I'm done but that's not like as data Engineers we need to be caring about efficiency did y'all know that the amount of carbon that's produced by all of data processing is like four or five% of like all the carbon right that we produce it's some very
(47:34) high percent of the economy that we should be working on and this is a great way to minimize the set so um I'm G to take a pause here and I'm gonna actually go and uh look at some comments and to see like what what's going on in the chat here so um uh anyways uh I'm going to be taking uh some questions from y'all if have any uh questions on this stuff on uh also I'm running a boot camp that starts in May and it's going to be really awesome and that's going to be another great place uh to learn a lot more about this in depth as opposed to
(48:14) just these kind of like oneh hour sessions where we kind of go back and forth but one of the things I wanted to show real quick here is if you want to join my boot camp and get a good discount if you do early bird V4 um at checkout uh you'll get 20% off and that will be uh pretty awesome so that can another great way to uh get more savings and uh then so this is gonna will make it this will make uh 1,600 to join uh where which is cheaper than all of my other boot camps uh come like all of them like the this it'll be
(48:49) best value that you can get but um definitely uh oh we got a question sorting queries even in a CT te make it more efficient uh no you like a lot of the times like the Sorting really should only happen at the very end sort should happen on right not any other time because if you sort can do the sort on right that is when you're going to get the most savings without a lot of the other minuses like because a lot of times sorting like inside the query is going to be uh kind of painful because like you'll sort it and then
(49:21) you'll join and then if you Shuffle happen s and then all of the Sorting that you just did is gone right it's gone right because just Shuffle happened bada bing bada boom right so um uh yeah this is going to be uh essentially like how a lot of this works but I I'm definitely down for like another uh probably another like fiveish minutes to answer any other kind of questions yall have whether it be about data Lakes expert.
(49:55) i data engineer the boot camp anything like that I'm really here for y'all to like really answer any questions I'm really grateful for y'all showing up to this uh live today I feel very humbled and happy that we were able to get over a hundred people in at the same time what file size do you think should remove a partition say you have year month day hour minute at what point do you remove the minute or even hour so like I've never ever seen a table freaking partitioned on hour or minute minute is insane partitioning your data on minute is absolutely
(50:26) bananas um generally speaking in big Tech like 95% of your data sets are partitioned on date like just and then uh some of them are partitioned on date and hour you it's not really about data volume here that really determines how you want to partition it it's more about um uh like data getting PE access patterns right because most people are going to want look at one day of data it's not really about volume right the volume doesn't determine whether you partition it's like how are people querying and using this data and a lot of times that's like
(50:59) they want to look at one day of data it's all about like your partitioning should be informed based on how your analysts run their wear statements right that's going to be the biggest thing is like what the wear Clause that's happening because partition pruning and predicate push down is what happens to make your queries a lot faster because then you ignore a lot of the other data that that's working right so um uh this someone said I was asking Delta format versus par which one is better uh it's the same it's so similar that
(51:33) the they're the same like I would not say that like it's the same uh Delta Delta Iceberg and huie it's the same like it's all the same um at Netflix Airbnb Facebook how do analytics Engineers use Python they it to build pipelines they use it to um Define metrics second my freaking camera is I'm trying to like figure out the this camera has like this weird autofocus problem I'm trying to figure it out but um uh analytics Engineers they write it to yeah do data models to write pipelines airflow code all that kind of
(52:07) stuff right oh there we go there we go okay that's good and so uh that that's what I would say uh yeah definitely uh okay so on the topic of partitioning when date is preferred but a date time is the only uh column in table is it worth processing cost of adding a date field then partitioning on that actually you know what's really cool about um uh iceberg is that you don't have to add another column for the partitioning so like for example if you wanted to create a table so I'm just going to create some random table here I
(52:38) was goingon to say test and then I'm going to say we have uh um we're gonna have a column here test varchar and then we have another column here uh Event Event Tim stamp which is a time stamp right and then we can do our little width here and then we can say uh partitioning um equals and then array and then you can say um inside of here you can say uh you Day of event Tim stamp so you can actually partition so like you're totally right it's very very common that you don't have like a date column so there's two ways to do
(53:11) this like one is you can add a date column and make that one way to do it another way to do it is you can uh can do it this way I think it's actually days I want to say it's days let's uh let's run this query real quick I got to put the format in there so say equals Park and then we have that let's see I think that this query should run it will yell at me if not is it not uh oh this already exists I love it okay let's uh days partitioning I love how that like already exists that's my favorite thing in the world okay so
(53:43) uh so I think okay is it day must be day is it just [Music] day there we go so now you'll see this is this will partition your data by day but you don't actually have to add that extra day column into the table because you can specify it here I don't like I know that there's probably a way to do that in Delta and huie I don't know how to do that in Delta and huie but I know you can but this is how do it in iceberg is like you can specify your partitioning with this day column so if you only have like an hour or
(54:16) minute Tim stamp then you can uh still partition your data that way without having to do some like adding like event date and then these two are like the same thing this is just like a trunk this is truncated version of this guy and that's a stupid thing to do right because then then you're just adding extra storage space and that's like not what you want to do right that's why H partitioning is going to be a better play for sure um so uh we say here um uh I got another two or three minutes here that we can uh
(54:45) go over things uh and just kind of like I I love talking about this stuff like uh like data infrastructure efficiency is probably one of my favorite uh topics to talk about okay say but if we have multiple joins on the same sort column does it make sense to sort in CTE great question Rama no actually again like the sort does not actually get registered like you think it would and in some cases like uh in spark right for example in spark there's a thing called a bucket join but if you're gonna like that doesn't happen in memory like
(55:19) if you want to leverage sorted columns on join in a data Lake environment obviously not talking about big query I'm not talking about snowflake those things have their own thing I'm talking about just data Lake Delta Iceberg huie like if you're talking about that stuff and you're wanting to do uh um like a special optimized sorted join that needs to happen on right so like you know how we did the little sorted by and then we write out the data that's where uh like trino and Spark all these things can leverage that fact because once the data
(55:53) is in Ram uh the spark does not trust that it stays sorted only trusts the partition metadata on whether or not that partition is sorted that's how it does right what is the difference between a categorized partition and run length coding they're very different run length and coding uh shrinks the data based on how many uh values show up in a row whereas um categorized partition is just that's the folders that you put data into uh they both work pretty well at minimizing the data sets that you're trying to create but like highly
(56:31) recommend uh learning both of those things if you want to be an efficient data engineer so you say uh what is the best relationship database where these compressions and partitioning uh makes much benefits like Oracle or SQL Server MySQL like I don't know man I don't work with that like I don't know like mean all of those are about the same like in those cases you're going to want leverage indexing and all that stuff this stuff does not apply as much in that space that was one of the things I started this presentation with like
(57:01) don't think that this works in uh relational database world don't think that this works in uh like snowflake or big query some of these concepts are similar in Snowflake and big query but they're different as well like uh this is more like this is all about Jeff bezos's computer and putting files on Jeff bezos's computer and leveraging that that is where you get the biggest bang for your buck is there so let's check if I understood it well run length and your optimization is about writing data which I have to partition and sort
(57:35) the data when we are reading it Should Skip sorting data right yeah for sure because Maro marillo the data is already sorted it's on right you don't have to worry about sorting it again because it's already sorted and it's not just sorted but like the partition metadata tells the processor that it's sorted and partition metadata is something that the processors can trust so that is one of the best ways to do it right uh what is the best way to learn uh distributed compute joining my boot camp and uh because we cover all of this stuff in a
(58:09) lot of detail so uh that is the best way to learn distributed compute and I'm going to be I'm going covering a lot more sessions on distributed compute trino query plans optimization skew every time 500 P PM uh 5: to 6 Pacific I'm going to be doing these lives consistently over the next long time like so just there's also all this free content that I'm giving you'all for free but if you want more in-depth content that's more than one hour a week then I highly highly recommend that you join my boot camp and I'm G to put the uh like gonna
(58:41) put this last one more time here before we go so if you do early bird V4 at checkout you get 20% off uh but have to do it by February 9th right 9th you have to until next Friday right that's when you can uh that's you can join and you be a part of the community and that's when we do eight hours a week of technical discussions like this for freaking six weeks and then we talk data quality distributed compute data modeling all of it all of it and so uh definitely that's going to be the probably one of your best ways to do it and uh awesome
(59:18) everyone thank you so much for uh joining my stream today I really appreciate it I am really excited for where things are going to go with the stuff but I hope that y'all have a fantastic Thursday and happy February
