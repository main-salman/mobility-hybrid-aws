https://www.youtube.com/watch?v=QxbaIeyUel8
https://developers.arcgis.com/javascript/latest/visualization/high-density-data/


all right uh welcome to this session this is best practices for building web apps that visualize large data sets and um jeremy bartley and uh christian eckness also contributed to this presentation and we have the question and answer let's go ahead and get started okay in this session i will cover some initial considerations what the javascript api and arcgis platform do for you how you can optimize your apps and data for better performance by choosing the right layer type looking at the different visualization
0:40
techniques that we have to offer and we'll look in an advanced case on data prep and pre-processing of very large data sets um considerations before you start really you really want to know your audience um are you building an application or a map that's going to very interested parties specialized users they're likely going to wait and they're going to um be patient for data to be loaded but if your goal is to communicate information to the general public and then you really don't know if they're going to be looking at the map that you create on
1:28
your phone on a phone or on desktop then um you really should simplify the map and really try to optimize the performance so uh are you targeting just a few specialized users are you um building an app for the public does the app have potential to go viral or the map um the devices question is very important today you know most of the general public is going to view the map on their phone or a tablet phones and tablets are great but they don't have as much processing power as a desktop machine so you can do you really in a lot of cases will have
2:13
less powerful machines consuming your map especially for information that goes to the public now you're building a map that like i said before that you're going to share to interested parties who are going to view it for their work on desktop they're usually going to be on a desktop and usually going to be fairly powerful and then you need to know think about the generalization choose an appropriate geometry type for the scale that you're looking at try to reduce those number of features reduce the overall vertices a lot of
2:44
this happens for you behind the scenes let's drill into some of that first off is feature layers are really your simplest bet and we'll talk about more about that in a minute but i'm going to talk you now about why that's a good option feature layers are powered by dynamic feature tiles this is an efficient flexible and scalable way to bring data to the client so here we've got a map of nearly a million building footprints and these are all drawn in on a web application how do we query all that data well query for it
3:28
one tile at a time what's important to note is that the javascript api is going to only is query the data in predictable ways so it's going to query based on an envelope that's going to be the same no matter what user is looking at the app and what device that it is allows caching to take place further down the stack which we'll talk about in a second javascript api is also smart to only request the fields that it needs so in this case it's going to request the object id field because that's the unique identifier
4:02
and the construction year because that's what's being rendered javascript api also quantizes your data so we request we'd like to be able have no more than one vertex per pixel on the screen so in this case we're quantizing the data set down to 38.2 meters we're also giving the server a hint that this is a tiled request and it's okay to start caching it there's a lot of goodness that you see on the responses just to call out a few of those one is that the data is i should step back for just a second we're also requesting the data in
4:45
protocol buffer format so this is a binary format that is really optimizes the size of the data and brings in so that it can be accessed efficiently on the client so there's good things you'll see a lot of good things if you were to look at the network tab of your host of feature services in arcgis online one is that everything over http 2.
5:07
everything is over https the sizes are fairly small on a per query basis these times are excellent and we see that we're actually getting hits from cloudfront so hit from the cdn and it's also being cached within arcgis online itself so if the cdn ever does not have latest version it gets cached there too and this data structure that comes back is highly efficient we've been working on this for many years and this is the most compact way to represent future geometry uh just to call a little bit of attention here
5:46
uh to give you a look under the hood uh when we quantize uh that data we're turning in uh floating precision into integers so we've got some uh quantization information here about uh what's the scale and uh what's the origin of transformation and then we've got the first point in integer space and then offsets from each point thereafter so this takes what would be and usually a very large precise number upwards of 15 digits and per coordinate per x and y reduces it into a very small tiny compact integer the javascript api is also efficiently
6:33
going to process and prepare those features to be drawn on the graphics card we've done a lot of work behind the scenes to optimize how lines are done so that we balance the display quality with overall performance so if we think that it's not going to be noticeable even though we have a very flexible symbol specification to allow you pick different types of joins these joins can be more expensive if the thickness of the line is small enough you really can't tell the difference between a quad join or any of the other join types
7:09
and and there's an advantage to doing that the processing time is much smaller for a join like this versus these more sophisticated joints so that's all happening under the hood and you'd see a lot of great performance increases uh in the javascript api if you've been following along for the last couple of years what we're looking at here are building footprints in los angeles and what we have on the right is came out last fall and what we have on the left was was there before that this is about i think a half million
7:44
building footprints something like that and you see how fast that data is paged in on a per tile basis on the right now you want to use different have different types of layers available to you and the most important part is to use the right layer for the job let's spend just a little bit of time talking about each these layer types feature layer in almost all cases is the best layer for you to use it's built on the notion of shareable queries across users and across apps so the javascript api will take care of that
8:26
for you when using the feature layer for visualization it will send the same request no matter where you are and that allows arcgis online to cache things at cdn if it's public and also caching things within arcgis online itself to avoid having to do expensive database calls that all happens behind the scenes you don't even really know that it's happening what you do notice is that the first time you visit a layer that performance is going to get better as the layer is used and it's little to no effort manage these types of layers
9:08
works great for interactive apps i'm going to take a pause here second before talking about the other types of layers and let's look at an example so here we've got vancouver building footprints um if we look at this i got 124 000 building footprints notice there's lots of attributes involved here what i'd like to show you first is a comparison between the classic map viewer and the new map viewer so first let's get down and uh take a look at this i'm gonna go ahead and drop this network tab at the bottom i've got it uh filtering out
9:54
um to just show me feature service uh responses and let's turn this on so we'll see a bunch of queries are happening and uh 32 queries happened just by me checking that box um the times actually you know it's fairly fast but uh that was 13.1 megabytes of data that was transferred so quite a bit of data for those 120 odd thousand uh responses if we look at this um let's pop this up a second what we'll see is that a couple things if you look at the network tab this feature tiles compress is true that means this is cached within
10:44
arcgis online and the times overall are pretty good you know we're looking at even though these are fairly large responses we're seeing times of around half a second and if we look at it an individual request what's in green out of that half second or so the green is how long it took the server to respond to this request so it took 156 milliseconds to respond to this query request and it took 300 milliseconds to download that data a couple other things to call out is the 3x api would request data request everything by default so
11:29
it would request star for outfields that means you're getting a lot of of attribute data you're getting all the attribute data even though you may not use it okay so let's let's compare this with um the new map viewer let me turn on developer tools and i'm going to turn on this feature layer right here wow that was quite a bit faster and it was less than a megabyte of data transferred if we look at some of these requests here you're gonna see that uh it's all coming from feature tiles we're not getting the benefit of the cdn
12:16
because i forgot to say this layer is uh private it's only accessible to me so we don't use the cdn when data is private and what we see here is that i relatives much smaller sizes actually if i could pop back here um it's quite easy to see sizes of like 400 450 or so kilobytes now we're looking at about 45 kilobytes and the main reason that is because both are using protocol buffer format the main reason is down here in terms of uh fields in this case we're only requesting the fid field uh that's the object id field in this
13:00
case we're not requesting any other attributes because i'm not using it um now what happens if your app needs to request that data in the future like you might need an attribute for a pop-up it all comes in dynamically so that's all taken care of under the hood okay now let's uh explain a little more about what we mean about feature tiles and what's happening under the hood so i'm going to go back to my table here and this is the raw data we see that it was last updated on february 19th let's make an edit to one of these
13:44
and i'll just change it back because i don't really want to mess with this data keep it correct but i really just wanted to trigger a data update so now the data has been updated as of february 21st at this time if i go back to this and let's reload this now let's turn that feature layer on what you're going to see is that it's going to load slower and that's because that feature tile had been invalidated because i made an edit to the data and see that it's almost loaded all data still the same amount of size 964
14:28
but these query times are much longer see we're seeing query times um upwards of three seconds now if i look at these uh requests it's all happening on the server so that this query particular query took six and a half seconds and uh the time to first byte that means the time that's running on the server was almost the entire amount 6.
14:54
48 seconds and the download time was only 52 milliseconds so that's the um that's what happens when you're going straight to the database and you're not able to leverage the power of those feature tiles now what happens if we um reload this and load it again what you're going to see is that this loads much faster see all of those feature tiles come back as true that means it's cached it within arcgis online and it can return that back to me in a much faster time and we see that the time here is much less on the waiting on server you know
15:48
whereas we were seeing six seconds before five seconds now we're seeing you know 300 milliseconds it's the power that's happening now let's take it one more step forward uh further let's go back and share this data set to the public um something i want to call your attention to it's always important to think about this hey on editing don't do it unless you really want to edit your data keep it non-editable because when you turn editing on you lose a lot of the benefits feature tiles things are slower you lose the ability to quantize the data
16:27
it's really important for you to only make layers editable when you have a very specific data collection purpose it doesn't mean you can't update the data like i just did but it's uh definitely important something to watch now the next thing i want to call your attention to is cache control so when a layer is shared to the public we leverage the power of a content delivery network and by default it's up to 30 seconds that means it'll once it's cache at the cdn it won't even go back to arcgis online for 30 seconds
17:03
and you can set this as high an hour um if you're and you know if your data um if you're updating your data every 15 minutes i would set this to 15 minutes and let it be uh um 15 to 20 minute delay on getting the latest updates it's just a good practice to do for this demo i'll just leave it at 30 seconds now let's go back reload this guy now what we're going to see is the query times are going to be similar to what we saw before because feature tile cache still comes into play but uh and i still see them miss from
17:52
cloudfront that's our cdn let's do it one more time okay now see all those hits from cloudfront and uh things even loaded faster than they did before and what's important is now look at these times these are now sub 50 milliseconds we see the response is that i've got a public cache control set and it's to 30 seconds and i see that get a hit from cloud front and what that means is now the time that you're gone you're waiting for that data has been greatly diminished we're looking at you know um 30 milliseconds to get 53
18:51
kilobytes of data and uh instead of the time to first byte being in the uh 300 millisecond range when it's at the cdn because is globally distributed you're going to go to the closest cdn endpoint in this case i only had to wait 17 milliseconds to get my first data set and to first byte from the cdn and then i downloaded whole thing in five milliseconds those are amazing times that's exciting okay so that's going on under the hood let's switch back to the presentation so feature layers are really great they're dynamic
19:31
they have all the data if you're building a javascript api application feature layers are amazing because you can do all these dynamic queries against those feature layers everything is drawn on the user's machine it's not a pre-generated tile that's great for interactive applications it's just it's just a great option next up is raster tiled layers you can pre-generate raster tiles of the data and that is really going to be your best performance raster tiles because on the client they don't have to do any work
20:09
display a raster image it's essentially an image that just gets drawn on the graphics card versus a feature layer or vector tile layer whose needs to be processed once that raw data is delivered to the browser it needs to be processed in memory and then turned into textures that are drawn on the graphics card so raster tiled layers are going to be your best bet if people have very slow computers very devices if you're really concerned about the smallest amount of data download and and the best possible performance across
20:54
any device or computer i think raster total layers may be that although with feature layers um it's uh going to be hard get much better than that 30 milliseconds that we were seeing there within the arcgis stack you can still do pop-ups and legends off of raster tiled layers and probably the biggest thing is that with raster tiled layers visualization cannot be controlled after it's built so that means if you rotate the map labels are going to be fixed kind of burnt into the image you can't do any kind of dynamic applications with it or explore that
21:40
data next up is vector tiled layer and with vector tile layer you can pre-generate vector tiles in arcgis online i forgot to say with a feature layer in arcgis online you can build raster tile layers or vector tile layers vector tiled layers are very similar uh to are similar in some ways feature layers and raster tile they are pre-generated tiles but they're not images they're actually data so they have that same kind of uh data response that the feature layer does uh the data gets chopped up quite a bit though so
22:20
at the tile boundaries uh at geometry passes that boundary it's going to be chopped right there and so you might end up with lots of different graphics that represent an individual feature and vector tiles include whatever geometry plus whatever attribute you decide to build them with the the benefit of vector tile layers is that they're very dynamic so just like with feature layers if you were to rotate the map you would see labels rotate they do allow some level of control over the visualization of style that's why you generally see base maps
23:02
being generated with vector tiles that allows to give very crisp uh visual quality and it allows um lots of different base maps to be created off of the data in an individual tile so like for arcgis online base maps instance the esri base maps are there's one set of tiled data with many different visualization styles a downside when you're talking about operational data is that you don't you don't have the ability to work with a legend um you'd have to make your own legend using a graphics package and no pop-ups as of right now i think
23:49
most important thing to get out of this session is that feature layer really the best all-around layer type for you it's the simplest um performance is great uh it allows you to be dynamic if you're building an application you can do so many things on the client with the feature layer and you can get very good performance with through feature tiles and one other point to call out that at 10 9 feature tiles are coming to enterprise and so a portal hosted feature service at ten nine will have the ability to do uh those same feature top so you get
24:27
that same benefit on enterprise which again on on the web okay now let's talk a little bit about different visualization techniques one is a pretty simple obvious one that probably most of you already know and that's scale dependent filtering and for this we're going to look at water distribution pipe example so these are water pipes in the thailand area there's about a half there's over million features and if you were to load this feature layer and draw all the features you'd be looking at you know almost 20 megabytes of data
25:17
that would have to be download downloaded if you filter it so like say the pipe size must be greater than 150 that's going to drop quite a bit and if you filter it even more aggressive like pipe size must be greater than 300 that's going to drop about 6 megabytes off and improve the time it's important to know you're still you're still downloading over 107 000 uh features so it's pretty it's awesome now let's take um a look at some examples here you can get actually before we look at the examples you can get even more
26:06
aggressive uh with this data set so for instance um if uh view resolution times two uh so um setting that pipe filter uh to uh be even higher you're going get even less features downloaded and the visual starts to degrade a little bit as if you if you were to uh go all the way here but you can get kind of a pretty good idea so all of this really is um uh it's just doing uh good cartographic defaults so if we um look at this map here i've gone ahead and done a little bit of authoring on it let me just pop the legend open
26:58
for a second um we're looking at um different types of pipes based on their material and the pipe size now if i look at the small scale view i'll see that the i've got a filter here and i'm filtering only showing things that are 300 and above in terms of size so i've selected 300 400 315 350 and so on i've also set a scale range so that at these scales i'll be drawing these features now um this is still quite a few features so let's actually look at the table that's still 107 000 features that are being drawn so quite a bit of data being downloaded
27:54
now as i zoom in things get crisped up the geometries start to get drawn in a little more detail i see that data coming in and do one more now i've switched i've switched to my large scale layer or i've not had where i don't have any filters applied at all and so now we see the full density of this data set and um really from a visualization perspective even this is a probably too much depends on what you're trying to show of course but um there's quite a bit of information going on here so you could even probably go
28:43
a little bit more aggressive and not turn all of the features on until you got to even the next level of detail we'll see that pipe scale has been set the large scale view sorry is going to show from from the town scale level all way to the room and you're going to get good performance you know all the way down with this view just keep zooming out a little bit farther yeah now i've switched to that other other view and i get kind of an overview the data set now um you can do obviously you can draw more so let me turn this guy off and turn
29:28
this one on this is all of the pipes without any um without any filter being set so it's drawing all of them it's just drawing all the same same way um it does benefit from all of that feature tile work that i talked about before but i just while we're here wanted to call out um what do i mean by feature layers are really dynamic um well it's just very dynamic so if i come over here and uh let's put pipe size back on again and let's do includes and only look at the ones where it's uh 300 you'll see uh as the attribute data gets requested
30:19
in for uh pipe size because i remember what i said uh the feature layer is smart and only requests the data that's needed now that i've applied this filter on the client side it's gone ahead and requested all that data and then look we've got it um now it'll be really quite fast so turn on the 400 200 250 315 350 you see what i mean um very dynamic fast that's what you get when you're working with a feature layer you have fast access to the data in browser on the client side so you can do some pretty amazing things with it
31:10
okay let's pop back over presentation here and let's talk about some other visualization techniques like clustering okay now from a performance perspective this is uh let's see how many points these are chicago home sales and there's 84 000 points so um you know feature layer can work pretty well with uh this data set let's just reload so we can see how fast that loads boom all the data is there 80 000 points but um let's spend a little bit of time on this visualization style what are we really trying to show here let's look at
32:01
something like um use the filter here sale price so these are home sales with for three years uh 2013 to 2016. and um let's just do color for a second here okay um and i have different themes so maybe i'm interested in the above and below theme okay the map's getting more interesting um you know smart mapping has gone in and sized my uh dots based on the scale so as i zoom in those dots will get bigger and so you'll be able to see them and we've colored them based on the sale price with a diverging ramp now that does look like a
32:49
better map but uh you know i think it's still um quite a bit of information is hidden in this and that's your goal right you don't want to just give out raw data your goal is to give out useful information and this is an example of giving out too much raw data and making it difficult to see the larger patterns but you can use these visual techniques like clustering let's turn that on to cluster that data and see how fast was that all happened on the client based on those 86 000 points and i can control the cluster radius so
33:32
i can essentially have clusters that represent very large areas or i can have clusters that represent much smaller areas and notice how the labels are dynamically being updated as i change this cluster radius obviously as i get make the radius lower those numbers are going to get smaller so the trick is for your data says to find you know find the right level of clustering and we've got some pretty nice tools now so i can control how the size range so how do i want the big features to be drawn um and the small features as well so i
34:16
can get them to try neatly fit most of the labels and you have control over these labels too so i can go in and by default it's labeling based on the count of the clusters but the map is still showing me sale price but maybe i want to switch this be the average sale price okay and it automatically added a k to make that fit better what's cool is it's just an arcade expression so if i wanted to add something like a dollar sign in front of it i can just do that now i got the dollar sign but i want you to go back and change my
35:06
size ranges a little they're bit too small and maybe i want to change my cluster radius out of fit a little bit better and smaller ones just a little bit smaller and make these guys a little bit bigger boom there we go and these are dynamic so as i zoom in uh clusters are going to be recomputed and i see all that information so um this is this example is just to really you know allow how you can improve the visual um experience of your maps by leveraging some of these visualization techniques that reduce the amount of information that's
35:51
being displayed and clustering is a great way to do that just for reference if i turn that off boom i see all the raw points and now i see the clustered view well also later on this year we'll be delivering out a solution where instead of computing these clusters on the client side which is what's happening here so i still have to download those 86 000 features we'll compute that cluster on the server and then have client work with the server generated clusters it won't be as dynamic but it will allow you to work on
36:32
extremely large data sets like in the millions hundreds of features but still be able to get a good visual out of it okay okay now you also may need to do some data prep and processing so that you can work with different geometry types that might also lead you to thin the geometries or the attributes out and i'll just talk about one example and then i'll give you a concrete example that christian put together last year so one example that you might think about in terms of data prep is let's say you were trying to build a
37:18
map of all the trees in your city now there can be you know hundreds of thousands of trees and um trees don't just in cities randomly appear right they're purposely planted so you can use techniques like aggregation so taking your large numbers of trees um and aggregating it into census block boundaries so instead of having a um each tree having its own feature you can have each block have the number of trees most common type of tree maybe the average diameter of the tree at breast height things like that that greatly reduces the amount of data
38:07
that's required to generate that map by aggregation and it also improves the visual and so i definitely recommend just thinking about different geospatial analytic techniques that you can do ahead of time to to reduce the complexity of your data so that you can visualize it effectively let's go into uh the one ocean app and talk about some of the data processing that that christian put into this app to build it um so this is the ecological marine units global oceans data said and we'll see here that this pretty
38:49
large data set this is four gigabytes um and uh that's very large and so you might be thinking well how i mean if was even just to download this data set it would take a long time to get that raw data even on my computer but we want to build a web application out of this we want people to be able walk up it and just use it um so the goal is to get total download as small possible and so you need to know your data do this so there this data set is has points at every quarter degree of latitude and longitude and then for each point it's got
39:32
up to 100 different depths with different attribute information at each location so um so there's up to 102 points of depth from surface down to 5000 meters and there are 12 attributes per point so uh that's a 52 million points and while you're not going to deliver 52 million points the browser and view it effectively so what can we do to get this as small possible step one is to reduce the number of geometries to have more just one point per location so we call this flattening the table pivoting generating a fat table
40:23
what we're going to do is take each one of those depth values for a particular attribute and we're going to pivot it so that those become now columns in the data that reduces it to like 677 000 features and the data is now one row per xy so an individual row has a single x y and there's only one x y in the entire data set and then each attribute or sorry each depth level for variable is now a column now the problem that we have here is now we've got over 1200 fields that's that's too much but we'll talk about that in a second
41:14
uh 600 000 is still quite a bit too 677 000 and uh so for this global visualization we went a christian went a step further and uh aggregated it from a quarter degree to half degree that brought it down uh to 84 711 features that's easy to work with in the browser and as you saw we were just working with about 85 000 points in the chicago sales example um by uh reducing the you can see difference between a quarter resolution uh quarter degree resolution and a half degree resolution so the turquoise dots are half degree
41:59
and the yellow dots are quarter degree so that's what we're pulling out out just the quarter degree or sorry the half degree version of this data set to get it to be as small possible still a lot of points now we need to reduce the number of columns uh thin those attributes out so for hosted feature services you can only have around a thousand fields um there's also uh the size of the data in each field will contribute to whether or not you can actually have up to a thousand twenty four it might be smaller um and uh christian went ahead
42:41
uh processed down this to reduce about 300 fields removed about 300 fields excuse me and by deciding which variable he wanted to pick and so on and what we see here is that um uh i have the velocity value at 79th depth point and then i've got the next velocity value at the 80th depth point and so on you'll see this just repeated at all the different depth points and we want to really control the number of fields that we get back and not have any extra because it's going to be quite a bit of data so one detail here is that
43:35
setting out fields to star is much better because it will make sure that all of your queries are http gets so that you leverage um the cdn and your browser cache but it um you want to control which fields come back when you specify star so christian reviews the view capabilities of arcgis online created a hostess feature view and picked the fields that he wanted and dropped everything else now that's going to result in a data set that only has about 300 fields and when star is used you're just going to get all 300 of those fields
44:23
set the cdn to one hour and now there's a manageable data set i can work with in the back in the browser what you can see is that this still loading a ton of data it's only 280 megabytes of data this is an example of app that's not usable for mobile devices the initial load time is a little bit longer but people are going to wait because they're interested let's take a look actually at this app and um definitely check out this blog post mapping large data sets on the web that describes the slides that i just went
45:02
through and written form and has a link to the app itself and so this app is really cool it's very dynamic see i'm moving my mouse around and i see that profile depth profile on the left so that's showing me at where my x y is all of the salinity values at different depths i can also view this map at different depths so as i'm moving this down you see that map is changing and that's because it's telling it to draw a different attribute and this is a very fast operation to do in the javascript api and you get very good performance and i
45:49
even switch between different uh variables like temperature or show the currents before i look at this and see that i've got uh each data point uh going in the direction of current of the of main current for this data set and then colored by its temperature and this is at different depths too and you can see interesting patterns that you might not have seen so yeah it's a great example of processing the data ahead of time to improve the overall visual so in conclusion the arcgis platform does a lot behind the scenes to optimize
46:36
your data and apps when loading large amounts of data but you could do more to optimize your apps for better performance by looking at different visualization techniques picking the right layer type for job doing some data prep and processing 